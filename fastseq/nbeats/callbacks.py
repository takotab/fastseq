# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_nbeats.callbacks.ipynb (unless otherwise specified).

__all__ = ['CombinedLoss', 'NBeatsChangeSizes', 'NBeatsLossPart', 'NBeatsBackward', 'NBeatsForward', 'BackwardSMAPE',
           'ForwardSMAPE', 'NBeatsTheta', 'NBeatsAttention', 'ClipLoss']

# Cell
from fastcore.utils import *
from fastcore.imports import *
from fastai2.basics import *
from fastai2.callback.hook import num_features_model
from fastai2.callback.all import *
from fastai2.torch_core import *
from torch.autograd import Variable
from ..all import *

from .model import *

# Cell
def CombinedLoss(loss_func, lookback, ratio = [1,1]):
    def _inner(pred, truth, *args, **kwargs):
        if len(pred.shape) == 2:
            pred = pred[:,None,:]
            truth = truth[:,None,:]
        if kwargs.get('reduction', 'mean') == 'none':
            loss = torch.zeros_like(pred)
            loss[:,:,:lookback] += loss_func(pred[:,:,:lookback],truth[:,:,:lookback], *args, **kwargs) * ratio[0]
            loss[:,:,lookback:] += loss_func(pred[:,:,lookback:],truth[:,:,lookback:], *args, **kwargs) * ratio[1]
        else:
            loss = loss_func(pred[:,:,:lookback],truth[:,:,:lookback], *args, **kwargs) * ratio[0]
            loss += loss_func(pred[:,:,lookback:],truth[:,:,lookback:], *args, **kwargs) * ratio[1]
        return loss

    return _inner


# Cell
class NBeatsChangeSizes(Callback):
    """Changes the lookback and horizon to match with the size of batch."""
    def begin_batch(self):
        l = self.xb[0].shape[-1]
        if l != self.learn.model.lookback :
            self.learn.model.lookback = l
            print(f"set lookback to {l}")
        h = self.yb[-1].shape[-1]-self.xb[0].shape[-1]
        if h != self.learn.model.horizon:
            self.learn.model.horizon = h
            print(f"set horizon to {h}")


# Cell
class NBeatsLossPart(Metric):
    "The loss according to the `loss_func` on a particular part of the time-serie."
    def __init__(self, start, end, name, *args, loss_func=None, **kwargs):
        store_attr(self,"start,end,loss_func")
        self._name = name

    def reset(self):           self.total,self.count = 0.,0
    def accumulate(self, learn):
        bs = find_bs(learn.yb)
        if self.loss_func is None:
            self.loss_func = learn.loss_func
        pred, truth = learn.pred, learn.yb[0]
        if len(pred.shape) == 2:
            pred = pred[:,None,:]
            truth = truth[:,None,:]
        assert pred[:,0,self.start:self.end].shape == truth[:,0,self.start:self.end].shape
        loss = to_detach(self.loss_func(pred[:,0,self.start:self.end], truth[:,0,self.start:self.end])) / truth[:,0,self.start:self.end].shape[-1]
        self.total += loss.mean()*bs
        self.count += bs
    @property
    def value(self): return self.total/self.count if self.count != 0 else None
    @property
    def name(self):  return self._name

# Cell
class NBeatsBackward(NBeatsLossPart):
    "The loss according to the `loss_func` on the backwards part of the time-serie."
    def __init__(self, lookback, *args, **kwargs):
        super().__init__(0, lookback, 'b_loss', *args, **kwargs)

# Cell
class NBeatsForward(NBeatsLossPart):
    "The loss according to the `loss_func` on the forward part of the time-serie."
    def __init__(self, lookback, *args, **kwargs):
        super().__init__(lookback, None, 'f_loss', *args, **kwargs)

# Cell
from ..metrics import *

class BackwardSMAPE(NBeatsLossPart):
    "The SMAPE on the backwards part of the time-serie."

    def __init__(self, lookback, *args, **kwargs):
        super().__init__(0, lookback, "b_smape", *args, loss_func=smape, **kwargs)


class ForwardSMAPE(NBeatsLossPart):
    "The SMAPE on the forwards part of the time-serie."

    def __init__(self, lookback, *args, **kwargs):
        super().__init__(lookback, None, "f_smape", *args, loss_func=smape, **kwargs)


# Cell
def _get_key_from_nested_dct(dct, s_key, exclude = [], namespace=''):
    r = {}
    for key in dct.keys():
        if sum([exc in key for exc in exclude])== 0 :
            if type(dct[key]) == dict:
                r.update(_get_key_from_nested_dct(dct[key], s_key, exclude, namespace=namespace+key))
            if s_key in key:
                r[namespace+key] = dct[key]
    return r

# Cell
class NBeatsTheta(Metric):
    "The sqaure of the `theta` for every block. "
    def reset(self):           self.total,self.count = 0.,0
    def accumulate(self, learn):
        bs = find_bs(learn.yb)
        theta_dct = _get_key_from_nested_dct(learn.model.dct,'theta',['bias','total','att'])
        t = torch.sum(tensor([v.float().abs().mean() for k,v in theta_dct.items()]))
        self.total += to_detach(t.abs().mean())*bs
        self.count += bs
    @property
    def value(self): return self.total/self.count if self.count != 0 else None
    @property
    def name(self):  return "theta"

# Cell
class NBeatsAttention(Callback):
    def means(self, df=True):
        theta_means = {k.replace('theta',''):v.float().cpu().data for k,v in _get_key_from_nested_dct(self.learn.model.dct,'theta',['total']).items()}
        ret = {}
        for k,v in theta_means.items():
            ret[k] = {}
            for i in range(v.shape[-1]):
                ret[k].update({'theta_'+str(i)+'_mean': v[:,i].mean().numpy(),
                               'theta_'+str(i)+'_std': v[:,i].std().numpy(),
                              })

        att = {k.replace('attention','att_mean'):v.float().cpu().numpy() for k,v in _get_key_from_nested_dct(self.learn.model.dct,'att',['total']).items()}
        for k in ret.keys():
            for att_key in att.keys():
                if k in att_key:
                    ret[k].update({'att_mean':att[att_key].mean(),
                                   'att_std':att[att_key].std(),
                                  })

        if df:
            return pd.DataFrame(ret)
        return ret

# Cell
class ClipLoss(Callback):
    def __init__(self, clip=5):
        self.clip = tensor([clip])

    def after_loss(self):
        self.learn.loss = torch.clamp(self.learn.loss, 0, self.clip.numpy()[0])

#     def after_backward(self):
#         nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)