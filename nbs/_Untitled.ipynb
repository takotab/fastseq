{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/dev/env37/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, NamedTuple, Optional, Any, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "# from gluon-ts/src/gluonts/dataset/repository/_util.py\n",
    "def to_dict(\n",
    "    target_values: np.ndarray,\n",
    "    start: str,\n",
    "    cat: Optional[List[int]] = None,\n",
    "    item_id: Optional[Any] = None,\n",
    "):\n",
    "    def serialize(x):\n",
    "        if np.isnan(x):\n",
    "            return \"NaN\"\n",
    "        else:\n",
    "            # return x\n",
    "            return float(\"{0:.6f}\".format(float(x)))\n",
    "\n",
    "    res = {\n",
    "        \"start\": str(start),\n",
    "        \"target\": [serialize(x) for x in target_values],\n",
    "    }\n",
    "\n",
    "    if cat is not None:\n",
    "        res[\"feat_static_cat\"] = cat\n",
    "\n",
    "    if item_id is not None:\n",
    "        res[\"item_id\"] = item_id\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def save_to_file(path: Path, data: List[Dict]):\n",
    "    print(f\"saving time-series into {path}\")\n",
    "    path_dir = os.path.dirname(path)\n",
    "    os.makedirs(path_dir, exist_ok=True)\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def metadata(cardinality: int, freq: str, prediction_length: int):\n",
    "    return {\n",
    "        \"freq\": freq,\n",
    "        \"prediction_length\": prediction_length,\n",
    "        \"feat_static_cat\": [\n",
    "            {\"name\": \"feat_static_cat\", \"cardinality\": str(cardinality)}\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# from gluon-ts/src/gluonts/support/pandas.py\n",
    "\n",
    "def frequency_add(ts: pd.Timestamp, amount: int) -> pd.Timestamp:\n",
    "    return ts + ts.freq * amount\n",
    "\n",
    "def forecast_start(entry):\n",
    "    return frequency_add(entry[\"start\"], len(entry[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# from gluon-ts/src/gluonts/dataset/repository/_lstnet.py\n",
    "def load_from_pandas(\n",
    "    df: pd.DataFrame,\n",
    "    time_index: pd.DatetimeIndex,\n",
    "    agg_freq: Optional[str] = None,\n",
    ") -> List[pd.Series]:\n",
    "    df = df.set_index(time_index)\n",
    "\n",
    "    pivot_df = df.transpose()\n",
    "    pivot_df.head()\n",
    "\n",
    "    timeseries = []\n",
    "    for row in pivot_df.iterrows():\n",
    "        ts = pd.Series(row[1].values, index=time_index)\n",
    "        if agg_freq is not None:\n",
    "            ts = ts.resample(agg_freq).sum()\n",
    "        first_valid = ts[ts.notnull()].index[0]\n",
    "        last_valid = ts[ts.notnull()].index[-1]\n",
    "        ts = ts[first_valid:last_valid]\n",
    "\n",
    "        timeseries.append(ts)\n",
    "\n",
    "    return timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# from gluon-ts/src/gluonts/dataset/repository/_lstnet.py\n",
    "class LstnetDataset(NamedTuple):\n",
    "    name: str\n",
    "    url: str\n",
    "    num_series: int\n",
    "    num_time_steps: int\n",
    "    prediction_length: int\n",
    "    rolling_evaluations: int\n",
    "    freq: str\n",
    "    start_date: str\n",
    "    agg_freq: Optional[str] = None\n",
    "\n",
    "\n",
    "root = \"https://raw.githubusercontent.com/laiguokun/multivariate-time-series-data/master/\"\n",
    "\n",
    "datasets_info = {\n",
    "    \"exchange_rate\": LstnetDataset(\n",
    "        name=\"exchange_rate\",\n",
    "        url=root + \"exchange_rate/exchange_rate.txt.gz\",\n",
    "        num_series=8,\n",
    "        num_time_steps=7588,\n",
    "        prediction_length=30,\n",
    "        rolling_evaluations=5,\n",
    "        start_date=\"1990-01-01\",\n",
    "        freq=\"1B\",\n",
    "        agg_freq=None,\n",
    "    ),\n",
    "    \"electricity\": LstnetDataset(\n",
    "        name=\"electricity\",\n",
    "        url=root + \"electricity/electricity.txt.gz\",\n",
    "        # original dataset can be found at https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#\n",
    "        # the aggregated ones that is used from LSTNet filters out from the initial 370 series the one with no data\n",
    "        # in 2011\n",
    "        num_series=321,\n",
    "        num_time_steps=26304,\n",
    "        prediction_length=24,\n",
    "        rolling_evaluations=7,\n",
    "        start_date=\"2012-01-01\",\n",
    "        freq=\"1H\",\n",
    "        agg_freq=None,\n",
    "    ),\n",
    "    \"traffic\": LstnetDataset(\n",
    "        name=\"traffic\",\n",
    "        url=root + \"traffic/traffic.txt.gz\",\n",
    "        # note there are 963 in the original dataset from https://archive.ics.uci.edu/ml/datasets/PEMS-SF\n",
    "        # but only 862 in LSTNet\n",
    "        num_series=862,\n",
    "        num_time_steps=17544,\n",
    "        prediction_length=24,\n",
    "        rolling_evaluations=7,\n",
    "        start_date=\"2015-01-01\",\n",
    "        freq=\"H\",\n",
    "        agg_freq=None,\n",
    "    ),\n",
    "    \"solar-energy\": LstnetDataset(\n",
    "        name=\"solar-energy\",\n",
    "        url=root + \"solar-energy/solar_AL.txt.gz\",\n",
    "        num_series=137,\n",
    "        num_time_steps=52560,\n",
    "        prediction_length=24,\n",
    "        rolling_evaluations=7,\n",
    "        start_date=\"2006-01-01\",\n",
    "        freq=\"10min\",\n",
    "        agg_freq=\"1H\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_lstnet_dataset(dataset_path: Path, dataset_name: str):\n",
    "    ds_info = datasets_info[dataset_name]\n",
    "\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    with open(dataset_path / \"metadata.json\", \"w\") as f:\n",
    "        f.write(\n",
    "            json.dumps(\n",
    "                metadata(\n",
    "                    cardinality=ds_info.num_series,\n",
    "                    freq=ds_info.freq,\n",
    "                    prediction_length=ds_info.prediction_length,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    train_file = dataset_path / \"train\" / \"data.json\"\n",
    "    test_file = dataset_path / \"test\" / \"data.json\"\n",
    "\n",
    "    time_index = pd.date_range(\n",
    "        start=ds_info.start_date,\n",
    "        freq=ds_info.freq,\n",
    "        periods=ds_info.num_time_steps,\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(ds_info.url, header=None)\n",
    "\n",
    "    assert df.shape == (\n",
    "        ds_info.num_time_steps,\n",
    "        ds_info.num_series,\n",
    "    ), f\"expected num_time_steps/num_series {(ds_info.num_time_steps, ds_info.num_series)} but got {df.shape}\"\n",
    "\n",
    "    timeseries = load_from_pandas(\n",
    "        df=df, time_index=time_index, agg_freq=ds_info.agg_freq\n",
    "    )\n",
    "\n",
    "    # the last date seen during training\n",
    "    ts_index = timeseries[0].index\n",
    "    training_end = ts_index[int(len(ts_index) * (8 / 10))]\n",
    "\n",
    "    train_ts = []\n",
    "    for cat, ts in enumerate(timeseries):\n",
    "        sliced_ts = ts[:training_end]\n",
    "        if len(sliced_ts) > 0:\n",
    "            train_ts.append(\n",
    "                to_dict(\n",
    "                    target_values=sliced_ts.values,\n",
    "                    start=sliced_ts.index[0],\n",
    "                    cat=[cat],\n",
    "                    item_id=cat,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    assert len(train_ts) == ds_info.num_series\n",
    "\n",
    "    save_to_file(train_file, train_ts)\n",
    "    print('saved train file')\n",
    "    # time of the first prediction\n",
    "    prediction_dates = [\n",
    "        frequency_add(training_end, i * ds_info.prediction_length)\n",
    "        for i in range(ds_info.rolling_evaluations)\n",
    "    ]\n",
    "\n",
    "    test_ts = []\n",
    "    for prediction_start_date in prediction_dates:\n",
    "        for cat, ts in enumerate(timeseries):\n",
    "            # print(prediction_start_date)\n",
    "            prediction_end_date = frequency_add(\n",
    "                prediction_start_date, ds_info.prediction_length\n",
    "            )\n",
    "            sliced_ts = ts[:prediction_end_date]\n",
    "            test_ts.append(\n",
    "                to_dict(\n",
    "                    target_values=sliced_ts.values,\n",
    "                    start=sliced_ts.index[0],\n",
    "                    cat=[cat],\n",
    "                    item_id=cat,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    assert len(test_ts) == ds_info.num_series * ds_info.rolling_evaluations\n",
    "\n",
    "    save_to_file(test_file, test_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving time-series into ../data/solar-energy/train/data.json\n",
      "saved train file\n",
      "saving time-series into ../data/solar-energy/test/data.json\n"
     ]
    }
   ],
   "source": [
    "generate_lstnet_dataset(Path('../data/solar-energy'), 'solar-energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,np.nan,np.nan])\n",
    "np.ones_like(a)*a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from fastseq.core import *\n",
    "path = Path('../data/m5/rows')\n",
    "i = 0\n",
    "for f in path.glob('*FOODS_2_1*_CA_*.json'):\n",
    "    f.copy('../data/m5_tiny/mini')\n",
    "# i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('../data/m5_tiny/m5_mini')\n",
    "meta = {\n",
    "    \"freq\": \"1D\",\n",
    "    \"prediction_length\": 28,\n",
    "    \"feat_static_cat\": [{\"name\": \"store_id\",},\n",
    "                        {\"name\": \"state_id\",}],\n",
    "    'feat_static_real':[{'name':'item_id',},\n",
    "                        {'name':'random'}],\n",
    "    'feat_dynamic_cat':[{'name':'weekday'},\n",
    "                        {'name':'month'}],\n",
    "    'feat_dynamic_cat':[{'name':'prices'},\n",
    "                        {'name':'dayofyear'}],\n",
    "}\n",
    "json.dump(meta,open(p / 'metadata.json','w') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freq': '1D',\n",
       " 'prediction_length': 28,\n",
       " 'feat_static_cat': [{'name': 'store_id'}, {'name': 'state_id'}],\n",
       " 'feat_static_real': [{'name': 'item_id'}, {'name': 'random'}],\n",
       " 'feat_dynamic_cat': [{'name': 'prices'}, {'name': 'dayofyear'}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(p / 'metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280 /400\n",
      "290 /400\n",
      "300 /400\n",
      "310 /400\n",
      "320 /400\n",
      "330 /400\n",
      "340 /400\n",
      "350 /400\n",
      "360 /400\n",
      "370 /400\n",
      "380 /400\n",
      "390 /400\n",
      "start 2011-01-29 2011-01-29 00:00:00\n",
      "target [2. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (1969,)\n",
      "feat_dynamic_real [[  2.88   2.88   2.88 ...   2.98   2.98   2.98]\n",
      " [ 29.    30.    31.   ... 169.   170.   171.  ]] (2, 1969)\n",
      "feat_dynamic_cat [[5 6 0 ... 4 5 6]\n",
      " [1 1 1 ... 6 6 6]] (2, 1969)\n",
      "feat_static_cat ['CA_1' 'CA'] (2,)\n",
      "feat_static_real [ 1.76000000e+02 -1.01814516e-01] (2,)\n"
     ]
    }
   ],
   "source": [
    "for i,f in enumerate(p.glob('*.json')):  \n",
    "    try:\n",
    "        ts = json.load(f)\n",
    "    except:\n",
    "        print('could not load',f)\n",
    "        assert False\n",
    "    if 'start' not in ts:\n",
    "        dct = {'start':'2011-01-29 00:00:00',\n",
    "              'target': ts['ts_con']['sales'],\n",
    "              'feat_dynamic_real':[ts['ts_con']['prices'],\n",
    "                                [(pd.Timestamp('2011-01-29 00:00:00') + pd.Timedelta('1D')*i).dayofyear for i in range(ts['_length'])]]\n",
    "                                         ,\n",
    "               'feat_dynamic_cat':[[(pd.Timestamp('2011-01-29 00:00:00') + pd.Timedelta('1D')*i).weekday() for i in range(ts['_length'])],\n",
    "                                           [(pd.Timestamp('2011-01-29 00:00:00') + pd.Timedelta('1D')*i).month for i in range(ts['_length'])]]\n",
    "                                         ,\n",
    "               'feat_static_cat': [ts['cat']['store_id'], ts['cat']['state_id']],\n",
    "               'feat_static_real': [int(ts['cat']['item_id'].split('_')[-1]), np.random.randn() ],           \n",
    "              }\n",
    "        open(f,'wb').write(orjson.dumps(dct ))\n",
    "        if i%10 == 0:\n",
    "            print(i, '/400')\n",
    "    \n",
    "for k,v in l[0].items():\n",
    "    print(k,v[:10], v.shape if type(v) == np.ndarray else v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-02-26 21:42:53</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-26 21:47:53</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-26 21:52:53</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-26 21:57:53</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-26 22:02:53</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     value\n",
       "timestamp                 \n",
       "2015-02-26 21:42:53     57\n",
       "2015-02-26 21:47:53     43\n",
       "2015-02-26 21:52:53     55\n",
       "2015-02-26 21:57:53     64\n",
       "2015-02-26 22:02:53     93"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv\"\n",
    "df = pd.read_csv(filepath_or_buffer=url, header=0, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env37",
   "language": "python",
   "name": "env37"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
